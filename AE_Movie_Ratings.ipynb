{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "600 epoch AE_Github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc6LqbuMLMdz"
      },
      "source": [
        "# AutoEncoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4hl6QtzM2zJ"
      },
      "source": [
        "# Part 1 - Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmJ8nDAKjfc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8HkY7iVM9hg"
      },
      "source": [
        "Importing the Training and Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T2VQ8L5Mx7y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "dd4c273c-d0bf-474e-8107-c6646f7c751c"
      },
      "source": [
        "test_set = pd.read_csv('/test_set.csv') \n",
        "train_set = pd.read_csv('/training_set.csv') \n",
        "\n",
        "train_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Movie</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>978302109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>978301968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>978300275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>978824291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>1287</td>\n",
              "      <td>5</td>\n",
              "      <td>978302039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750116</th>\n",
              "      <td>6040</td>\n",
              "      <td>1091</td>\n",
              "      <td>1</td>\n",
              "      <td>956716541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750117</th>\n",
              "      <td>6040</td>\n",
              "      <td>1094</td>\n",
              "      <td>5</td>\n",
              "      <td>956704887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750118</th>\n",
              "      <td>6040</td>\n",
              "      <td>562</td>\n",
              "      <td>5</td>\n",
              "      <td>956704746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750119</th>\n",
              "      <td>6040</td>\n",
              "      <td>1096</td>\n",
              "      <td>4</td>\n",
              "      <td>956715648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750120</th>\n",
              "      <td>6040</td>\n",
              "      <td>1097</td>\n",
              "      <td>4</td>\n",
              "      <td>956715569</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>750121 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        User  Movie  Rating  Timestamp\n",
              "0          1    661       3  978302109\n",
              "1          1    914       3  978301968\n",
              "2          1   3408       4  978300275\n",
              "3          1   2355       5  978824291\n",
              "4          1   1287       5  978302039\n",
              "...      ...    ...     ...        ...\n",
              "750116  6040   1091       1  956716541\n",
              "750117  6040   1094       5  956704887\n",
              "750118  6040    562       5  956704746\n",
              "750119  6040   1096       4  956715648\n",
              "750120  6040   1097       4  956715569\n",
              "\n",
              "[750121 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "61TeEbHa39eX",
        "outputId": "10261bf1-0ddb-42cc-8bf1-f3d37d5fa10b"
      },
      "source": [
        "test_set\r\n",
        "#so 250088 + 750121 = 1 million total ratings "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User</th>\n",
              "      <th>Movie</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>978300760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1197</td>\n",
              "      <td>3</td>\n",
              "      <td>978302268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>2804</td>\n",
              "      <td>5</td>\n",
              "      <td>978300719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>595</td>\n",
              "      <td>5</td>\n",
              "      <td>978824268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>938</td>\n",
              "      <td>4</td>\n",
              "      <td>978301752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250083</th>\n",
              "      <td>6040</td>\n",
              "      <td>3735</td>\n",
              "      <td>4</td>\n",
              "      <td>960971654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250084</th>\n",
              "      <td>6040</td>\n",
              "      <td>2791</td>\n",
              "      <td>4</td>\n",
              "      <td>956715569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250085</th>\n",
              "      <td>6040</td>\n",
              "      <td>527</td>\n",
              "      <td>5</td>\n",
              "      <td>956704219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250086</th>\n",
              "      <td>6040</td>\n",
              "      <td>2003</td>\n",
              "      <td>1</td>\n",
              "      <td>956716294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250087</th>\n",
              "      <td>6040</td>\n",
              "      <td>535</td>\n",
              "      <td>4</td>\n",
              "      <td>964828734</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250088 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        User  Movie  Rating  Timestamp\n",
              "0          1   1193       5  978300760\n",
              "1          1   1197       3  978302268\n",
              "2          1   2804       5  978300719\n",
              "3          1    595       5  978824268\n",
              "4          1    938       4  978301752\n",
              "...      ...    ...     ...        ...\n",
              "250083  6040   3735       4  960971654\n",
              "250084  6040   2791       4  956715569\n",
              "250085  6040    527       5  956704219\n",
              "250086  6040   2003       1  956716294\n",
              "250087  6040    535       4  964828734\n",
              "\n",
              "[250088 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGKVWAWJJEsu"
      },
      "source": [
        "Converting our Dataframes into Numpy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZhupHWA3pRm",
        "outputId": "17553948-c02c-4bdb-c214-b478fb28d876"
      },
      "source": [
        "train_set = np.array(train_set, dtype = 'int') #specified the data type for our data, which is all integers anyway since its ratings and IDs\r\n",
        "test_set = np.array(test_set, dtype = 'int') #specified the data type for our data, which is all integers anyway since its ratings and IDs\r\n",
        "\r\n",
        "train_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[        1,       661,         3, 978302109],\n",
              "       [        1,       914,         3, 978301968],\n",
              "       [        1,      3408,         4, 978300275],\n",
              "       ...,\n",
              "       [     6040,       562,         5, 956704746],\n",
              "       [     6040,      1096,         4, 956715648],\n",
              "       [     6040,      1097,         4, 956715569]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM_SKin0RDfI"
      },
      "source": [
        "Getting the total number of users and movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM1hsOkFT6ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "033f5879-aa1c-4cd9-fe6c-00e85c3fac93"
      },
      "source": [
        "num_users = int(max(max(test_set[:,0]),max(train_set[:,0]))) #getting the maximum user ID in the training and test set from all the rows, and column 0 which is the user ID\n",
        "num_movies = int(max(max(test_set[:,1]),max(train_set[:,1]))) #getting the maximum movie ID in the training and test set from all the rows, and column 1 which is the movie ID\n",
        "\n",
        "print(num_users) #so max userID is 6040 and max movieID is 3952\n",
        "print(num_movies)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6040\n",
            "3952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujeJIQnLVrDL"
      },
      "source": [
        "Converting Datasets into Array with 6040 rows (users) and 3952 columns (ratings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEn7Nwjo4vdz"
      },
      "source": [
        "#Here we're going to create a list of lists. Basically we'll have 6040 lists (# of users), and each list will have 3952 movies (with movies they haven't rated equal to 0). We do this with a numpy array to make it easier to work with pytorch afterwards\n",
        "def ratingslist(data):\n",
        "  ratings_list = []\n",
        "  for user_id in range(1,num_users+1):\n",
        "    #starting from 1 since first user_ID is 1 and ending on 6040+1 since python doesn't include the upper bound\n",
        "    movie_id = data[:,1][data[:,0] == user_id] #so here we're taking all the rows from column 1, which is the movie ids from the training/test set. We're also making it so that it only takes the movies watched by a specific user id. To do this, we basically took column 0 and said it must be equal to user_id from the training/test set\n",
        "    rating_id = data[:,2][data[:,0] == user_id] #so here we're taking all the rows from column 2, which is the ratings from the training/test sets. We're also making it so that it only takes the ratings by a specific user id. To do this, we basically took column 0 and said it must be equal to user_id from the training/test set\n",
        "    rating = np.zeros(num_movies) #creating a list of 3952 movies initialized with all zeros, so we can then populate it with ratings from users afterwards, and any movies they didn't rate will be given a value of 0\n",
        "    rating[movie_id - 1] = rating_id #did -1 because python index starts at 0, but movie_id starts with 1\n",
        "    ratings_list.append(list(rating)) #now adding the ratings of user_id to the ratings list\n",
        "  return ratings_list\n",
        "\n",
        "train_set = ratingslist(train_set) #now we're using our function to convert our training set into a list of lists\n",
        "test_set = ratingslist(test_set) #same above but for test set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWwFNAG449n2",
        "outputId": "6ad1e1b6-1d93-43bf-e1af-ab11b66503da"
      },
      "source": [
        "print(\"Number of Rows (users) =\",len(test_set)) #so it's 6040 users\r\n",
        "print(\"Number of Cols (ratings) =\", len(test_set[0])) #and the number of columns is 3952 ratings (1 for each movie, 0 if unwatched movie)\r\n",
        "print(\"User 1, Movie 661, Rating =\",train_set[0][660]) #we can see for user 1, movie 660, the rating given was 3 which reflects the first row in the training set when first imported\r\n",
        "print(\"User 6040, Movie 535, Rating =\",test_set[6039][534]) #we can see for user 6040, movie 535, the rating given was 4 which reflects the last row in the test set when first imported"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Rows (users) = 6040\n",
            "Number of Cols (ratings) = 3952\n",
            "User 1, Movie 661, Rating = 3.0\n",
            "User 6040, Movie 535, Rating = 4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUhzNLSHbghD"
      },
      "source": [
        "Converting Array into Torch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VQGKGC8ahNR"
      },
      "source": [
        "#so we could create a neural network with numpy arrays, but pytorch arrays are far more efficient, which is why we're using it\n",
        "#for autoencoders, pytorch is more simple to use than tensorflow\n",
        "train_set = torch.FloatTensor(train_set) #FloatTensor expects a list of lists\n",
        "test_set = torch.FloatTensor(test_set) #FloatTensor expects a list of lists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrpcRoJI5NhQ",
        "outputId": "045f1500-ffd6-4603-9057-ac376cad47fd"
      },
      "source": [
        "#Converting to Torch Tensor kept the shapes the same\r\n",
        "print(\"Number of Rows (users) =\",len(test_set)) #so it's 6040 users\r\n",
        "print(\"Number of Cols (ratings) =\", len(test_set[0])) #and the number of columns is 3952 ratings (1 for each movie, 0 if unwatched movie)\r\n",
        "print(\"User 1, Movie 661, Rating =\",train_set[0][660]) #we can see for user 1, movie 660, the rating given was 3 which reflects the first row in the training set when first imported\r\n",
        "print(\"User 6040, Movie 535, Rating =\",test_set[6039][534]) #we can see for user 6040, movie 535, the rating given was 4 which reflects the last row in the test set when first imported"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Rows (users) = 6040\n",
            "Number of Cols (ratings) = 3952\n",
            "User 1, Movie 661, Rating = tensor(3.)\n",
            "User 6040, Movie 535, Rating = tensor(4.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "try3evq0BnzW"
      },
      "source": [
        "# Part 2 - Creating the Class for Future Neural Network Objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7SUcVHixWXN"
      },
      "source": [
        "#stacked autoencoder\n",
        "#so here we're using inheritance, from PyTorch nn.Module. We do this so that we can use all the variables and functions from parent class Module, which will be useful for us when developing a SAE\n",
        "class stacked_autoencoder(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    #We put , then blank, as this will consider the variables of the module class because we are doing inheritance\n",
        "    super(stacked_autoencoder, self).__init__() #super is a function that allows us to use Module's functions and variables\n",
        "    self.fc1 = nn.Linear(num_movies, 32) #first full connection. num_movies is the number of input nodes (number of features in the input vector), and 32 is the number of hidden nodes\n",
        "    self.fc2 = nn.Linear(32, 16) #second full connection. 32 is the input for this connection, and 16 is the 2nd hidden layer\n",
        "    self.fc3 = nn.Linear(16, 32) #so now we're just decoding, so we're trying to determine the output with this next hidden layer\n",
        "    self.fc4 = nn.Linear(32, num_movies) #this is the output, so it finishes with the same number of nodes as the input layers\n",
        "    self.activation = nn.Sigmoid() #using the sigmoid activation function. Experimentation happened with the rectified linear unit activation function but the results were not as good\n",
        "  \n",
        "  #This next function is to create the forward propagations through our Stacked auto encoder\n",
        "  #x is our input vector of features \n",
        "  def forward(self, x):\n",
        "    x = self.activation(self.fc1(x)) #fc1 takes the input vectors x, which is then taken in by activation function\n",
        "    x = self.activation(self.fc2(x)) #encoding x into the 2nd hidden layer\n",
        "    x = self.activation(self.fc3(x)) #decoding x into the 3rd hidden layer\n",
        "    x = self.fc4(x) #now just getting the output\n",
        "    return x\n",
        "\n",
        "SAE = stacked_autoencoder()\n",
        "\n",
        "criterion = nn.MSELoss() #loss function which is MSE\n",
        "optimizer = optim.RMSprop(SAE.parameters(), lr = 0.01, weight_decay = 0.5) #Weight decay is used to reduce the learning rate after every epoch, and that is used to regulate the convergence and prevent overfitting (0.5 was found through experimentation)\n",
        "#Adam optimizer did not have as good results as RMSProp optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce0BgxZBMP1K",
        "outputId": "f0ee4f9a-f160-48b7-f186-b447a8809d43"
      },
      "source": [
        "SAE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "stacked_autoencoder(\n",
              "  (fc1): Linear(in_features=3952, out_features=32, bias=True)\n",
              "  (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
              "  (fc3): Linear(in_features=16, out_features=32, bias=True)\n",
              "  (fc4): Linear(in_features=32, out_features=3952, bias=True)\n",
              "  (activation): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX67S5zGjDXw"
      },
      "source": [
        "# Part 3 -Training our SAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzR3AufqM-lS",
        "outputId": "59394af6-26a2-4525-ce2c-8683d3b13b6f"
      },
      "source": [
        "#Adding fake batch dimension for PyTorch\r\n",
        "user6040 = train_set[6039]\r\n",
        "user6040_var = Variable(train_set[6039])\r\n",
        "user6040_fb = Variable(train_set[6039]).unsqueeze(0)\r\n",
        "\r\n",
        "print(user6040, len(user6040))\r\n",
        "print(user6040_var, len(user6040_var))\r\n",
        "print(user6040_fb, len(user6040_fb), len(user6040_fb[0])) #see how it's a nested array now, thus creating a batch of a single input vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 0., 0.,  ..., 0., 0., 0.]) 3952\n",
            "tensor([3., 0., 0.,  ..., 0., 0., 0.]) 3952\n",
            "tensor([[3., 0., 0.,  ..., 0., 0., 0.]]) 1 3952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugKj3ducRRNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0446a8de-7873-4507-cf2b-ecda1a66ba05"
      },
      "source": [
        "import time #To keep track of how long it takes to train our model\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "num_epoch = 500\n",
        "\n",
        "for epoch in range(1, num_epoch + 1):\n",
        "  training_loss = 0 #initializing the training loss\n",
        "  s = 0. #the . just makes it a float. s is going to be used to count the number of users who did not rate any movies, so that we don't perform any computation on them. Here we are just initializing it\n",
        "  \n",
        "  #below we're just using num_users to go from 0 to 6040 user indexes\n",
        "  for user_id in range(num_users):\n",
        "    #so below, first we're taking the index number user_id in the training set\n",
        "    input = Variable(train_set[user_id]).unsqueeze(0) #A NN in PyTorch and Keras cannot accept a vector of one dimension, but it does accept a batch of input vectors. So here we're adding an additional, fake dimension which will correspond to a batch, which will be index 0, so the first dimension. so when we unsqueezed our variable (train set input), we added a new dimension of index 0. All this creates a batch of a single input vector\n",
        "    target = input.clone() #so we're just cloning the input, so we can compare the output to the original input\n",
        "    \n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "       #if statement is checking if there is a user that has 1 or more rated movies\n",
        "       output = SAE(input) #running our input vector data for user_id through our SAE class, which then at the end spits out the output from the forward function\n",
        "       target.require_grad = False #this is just for optimization. We only want to apply Stochastic Gradient Descent only to the input, and not the clone which is target. So this require_grad function set to false just means we don't compute the gradient WRT the target\n",
        "       output[target == 0] = 0 #take the indexes of output when target data (which has the same indexes as output since it's just a clone of input) has no ratings (equal to zero) equal to zero. This way we don't take these empty inputs indexes in the computation of the error, so they won't impact the updating of the weights.\n",
        "       loss = criterion(output, target) #the loss just compares the MSE of the vector of inputs (target) vs the output\n",
        "       mean_corrector = num_movies/float(torch.sum(target.data > 0) + 1e-10) #calculating the average number of movies that had a non-zero rating and adding 1e-10 just incase the denominator is 0. We need this mean corrector to represent the average of the errors, but by only considering the movies that were rated\n",
        "       loss.backward() #this just calls the backward method for the loss, and it will tell in which direction we need to update the different weights (i.e do we need to increase or decrease the weights)\n",
        "       training_loss += np.sqrt(loss.data*mean_corrector) #computing the error by square rooting the mean squared error, which is in index 0 of the loss data and is adjusted by multiplying the mean correction\n",
        "       s += 1. #adding to the number of users that rated at least 1 movie\n",
        "       optimizer.step() #step is just a method of using the optimizer from the RMSprop class. Backwards decided the direction in which to change the weight, whereas optimizer step decides the magnitude of said increase or decrease\n",
        "  if epoch%25 == 0:\n",
        "    print('epoch: ' + str(epoch) + ' loss:' + str(training_loss/s))\n",
        "\n",
        "print(f'Training took {(time.time() - start_time)/60} minutes')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 25 loss:tensor(0.9749)\n",
            "epoch: 50 loss:tensor(0.9470)\n",
            "epoch: 75 loss:tensor(0.9171)\n",
            "epoch: 100 loss:tensor(0.9063)\n",
            "epoch: 125 loss:tensor(0.8942)\n",
            "epoch: 150 loss:tensor(0.8797)\n",
            "epoch: 175 loss:tensor(0.8722)\n",
            "epoch: 200 loss:tensor(0.8631)\n",
            "epoch: 225 loss:tensor(0.8585)\n",
            "epoch: 250 loss:tensor(0.8540)\n",
            "epoch: 275 loss:tensor(0.8487)\n",
            "epoch: 300 loss:tensor(0.8444)\n",
            "epoch: 325 loss:tensor(0.8396)\n",
            "epoch: 350 loss:tensor(0.8362)\n",
            "epoch: 375 loss:tensor(0.8316)\n",
            "epoch: 400 loss:tensor(0.8289)\n",
            "epoch: 425 loss:tensor(0.8222)\n",
            "epoch: 450 loss:tensor(0.8181)\n",
            "epoch: 475 loss:tensor(0.8131)\n",
            "epoch: 500 loss:tensor(0.8077)\n",
            "Training took 234.49684039751688 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwx8llQ7uJs"
      },
      "source": [
        "# Part 4 - Testing our Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U-1wqVgiSSY",
        "outputId": "bc8e277c-bb60-43e3-dd98-b93de01f86bd"
      },
      "source": [
        "test_loss = 0 #initializing the test loss\n",
        "s = 0. #the . just makes it a float. s is going to be used to count the number of users who did not rate any movies, so that we don't perform any computation on them. Here we are just initializing it\n",
        "  \n",
        "#below we're just using num_users to go from 0 to 6040 user indexes\n",
        "for user_id in range(num_users):\n",
        "  #so below, first we're taking the index number user_id in the training set\n",
        "  input = Variable(train_set[user_id]).unsqueeze(0) #so we keep the training set, because the training set is the input that will be used to activate the hidden neurons for our test set predictions. Because the training set does not have any of the ratings from the test set, we have to use the training set as the input so it can try to predict the ratings of answers it does not contain, then compare it afterwards with the test set results\n",
        "  target = Variable(test_set[user_id]).unsqueeze(0) #so here, the target is the test set, which makes sense because we're going to compare our predicted training set results from input with our test set\n",
        "\n",
        "  if torch.sum(target.data > 0) > 0:\n",
        "    #if statement is checking if there is a user that has 1 or more rated movies\n",
        "    output = SAE(input) #running our input vector data for user_id through our SAE class, which then at the end spits out the output from the forward function\n",
        "    target.require_grad = False #this is just for optimization. We only want to apply Stochastic Gradient Descent only to the input, and not the clone which is target. So this require_grad function set to false just means we don't compute the gradient WRT the target\n",
        "    output[target == 0] = 0 #take the indexes of output when target data (which has the same indexes as output since it's just a clone of input) has no ratings (equal to zero) equal to zero. This way we don't take these empty inputs indexes in the computation of the error, so they won't impact the updating of the weights.\n",
        "    loss = criterion(output, target) #the loss just compares the MSE of the vector of inputs (target) vs the output\n",
        "    mean_corrector = num_movies/float(torch.sum(target.data > 0) + 1e-10) #calculating the average number of movies that had a non-zero rating and adding 1e-10 just incase the denominator is 0. We need this mean corrector to represent the average of the errors, but by only considering the movies that were rated\n",
        "    #we removed loss.backward because we don't need to update weights since we're not performing backpropagation on the test set, we're just predicting in one go\n",
        "    test_loss += np.sqrt(loss.data*mean_corrector) #computing the error by square rooting the mean squared error, which is in index 0 of the loss data and is adjusted by multiplying the mean correction\n",
        "    s += 1. #adding to the number of users that rated at least 1 movie\n",
        "    #we removed optimizer because we don't need to update weights since we're not performing backpropagation on the test set, we're just predicting in one go\n",
        "print('test loss:' + str(test_loss/s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss:tensor(0.8958)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
